# Ch1-Project_1B_Project_Template
# ğŸµ Project: Data Modeling with Apache Cassandra â€” Sparkify

## ğŸ“Œ Project Summary

Sparkify is a music streaming startup that collects user activity data in daily CSV files.
The analytics team wants to analyze **what songs users are listening to**, but the current data format does not support efficient queries.

In this project, an **ETL pipeline** is built in Python and **query-driven data models** are designed in **Apache Cassandra** to support fast analytical queries on song play data.

This project demonstrates:

* NoSQL data modeling using Cassandra
* ETL processing of raw CSV files
* Query-optimized table design
* Data loading and validation using SELECT queries

---

## ğŸ¯ Business Questions Answered

The Cassandra tables are designed to answer the following queries:

1. **Songs played in a session**
   â†’ What songs were played in a given session, and in what order?

2. **User and song by time in session**
   â†’ Who listened to what song at what time in a session?

3. **Songs played by a user**
   â†’ What songs has a specific user listened to?

Each query is supported by a **separate table**, following Cassandra best practices.

---

## ğŸ“‚ Dataset Description

The dataset consists of multiple CSV files partitioned by date:

```
event_data/
â”‚
â”œâ”€â”€ 2018-11-08-events.csv
â”œâ”€â”€ 2018-11-09-events.csv
â”œâ”€â”€ ...
```

Each file contains user activity logs such as:

* userId
* firstName, lastName
* sessionId
* song, artist
* itemInSession
* timestamp
* page

Only records where:

```
page == "NextSong"
```

are used, since those represent actual song plays.

---

## ğŸ”„ ETL Pipeline

### âœ… Step 1: Combine CSV Files

The ETL process:

```
Multiple CSV files
        â”‚
        â–¼
Read + Filter (NextSong only)
        â”‚
        â–¼
Select required columns
        â”‚
        â–¼
event_datafile_new.csv
```

This produces a **denormalized dataset** used for Cassandra inserts.

---

### âœ… Step 2: Load Data into Cassandra

Rows from `event_datafile_new.csv` are inserted into three Cassandra tables, each created for a specific query.

---

## ğŸ§± Data Modeling Strategy (Query-Driven)

Cassandra requires that:

* Tables are designed **based on query patterns**
* Partition keys must appear in WHERE clauses
* Joins are not supported

Therefore:

* Each query has its **own table**
* Data is **duplicated across tables** to optimize reads

---

## ğŸ—ï¸ Table Designs

### âœ… Table 1 â€” Songs Played in Session

**Query:**
Get artist, song, and song length by session and item number.

**Table:** `songs_by_session`

**Primary Key:**

```
((session_id), item_in_session)
```

* Partition Key: `session_id`
* Clustering Key: `item_in_session` (keeps play order)

---

### âœ… Table 2 â€” User and Song by Time in Session

**Query:**
Get user, artist, and song by session and time.

**Table:** `users_by_session_time`

**Primary Key:**

```
((session_id), start_time)
```

* Partition Key: `session_id`
* Clustering Key: `start_time`

---

### âœ… Table 3 â€” Songs Played by User

**Query:**
Get all songs listened to by a user.

**Table:** `songs_by_user`

**Primary Key:**

```
((user_id), start_time)
```

* Partition Key: `user_id`
* Clustering Key: `start_time`

---

## â–¶ï¸ How to Run the Project

### âœ… Prerequisites

* Python 3.x
* Apache Cassandra running locally
* Cassandra Python driver installed

Install driver if needed:

```bash
pip install cassandra-driver
```

---

### âœ… Run Steps

1. Start Cassandra service
2. Open Jupyter Notebook

```bash
jupyter notebook
```

3. Open the project notebook
4. Run cells **top to bottom**:

   * ETL processing
   * Keyspace creation
   * Table creation
   * Data insertion
   * Query validation

Expected output:
Each SELECT query prints correct results matching the project questions.

---

## ğŸ§ª Validation Strategy

After inserts, SELECT queries are executed using:

* Exact partition keys
* No `ALLOW FILTERING`

This ensures:

* Efficient lookups
* Proper Cassandra modeling

---

## ğŸ§¼ Code Quality Practices

* Modular notebook sections
* Clear markdown explanations per query
* Correct data types used in CREATE TABLE
* Tables dropped before recreation for clean testing
* Meaningful table names reflecting query intent

---

## âš ï¸ Cassandra Concepts Applied

| Concept                 | Applied |
| ----------------------- | ------- |
| Query-first modeling    | âœ…       |
| Denormalization         | âœ…       |
| No joins                | âœ…       |
| Composite primary keys  | âœ…       |
| Partition-aware queries | âœ…       |

---

## ğŸŒŸ Optional Enhancements

Possible improvements:

* Batch inserts for performance
* Logging instead of print statements
* Dockerized Cassandra setup
* Conversion to Python scripts

---

## ğŸ“š What This Project Demonstrates

This project shows ability to:

* Design NoSQL schemas based on access patterns
* Build ETL pipelines using Python
* Apply Cassandra data modeling principles
* Translate business questions into database schemas

âœ… **Short project explanation for interviews**
âœ… **Architecture diagram (visual)**
âœ… **Final rubric compliance checklist**

Just tell me what youâ€™d like next, Sheetal ğŸ˜Š
